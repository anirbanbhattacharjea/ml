import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

class LinearRegressionGD:
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.theta = None
        
    def fit(self, X, y):
        # Add bias term to features
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        # Initialize weights randomly
        self.theta = np.random.randn(X_b.shape[1])
        # Perform gradient descent
        for i in range(self.iterations):
            gradients = 2/X_b.shape[0] * X_b.T.dot(X_b.dot(self.theta) - y)
            self.theta -= self.learning_rate * gradients
            
    def predict(self, X):
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        return X_b.dot(self.theta)
    
# Create an instance of LinearRegressionGD
model = LinearRegressionGD()

# Assuming you have X_train, y_train, X_test, y_test defined
# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Calculate Mean Squared Error (MSE) and R-squared (R2) for testing dataset
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2) Score:", r2)
